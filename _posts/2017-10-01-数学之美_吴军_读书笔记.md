---
layout: post
title:  "《数学之美》(吴军) 读书笔记"
date:   2017-10-01 10:00:00 +0800
tags: book Algorithm
---
吴军在美国硅谷谷歌上市前工作多年，上市后财富自由进入投资领域，在技术、理论、投资、教育方面都有建树。

此书是吴军之前在谷歌的一系列博客改写而成。

<br/>
### 第1章 文字和语音VS数字和信息
原始人需要的文字和数量描述都很少，所以没有发明文字和数字。

后来随着脑袋放不下这些语音和数量，就产生了文字和数字。最早的泥板象形文和骨头肋条上的刻线计数。

中国古代的日常用语和现在的口语类似，但是书写的文字由于带宽较低（书写介质成本高）所以使用压缩编码后的文字（文言文），世界各民族都是这样。

古犹太人抄写圣经时总会犯错，所以给每个文字对应一个数字，然后抄好一页后，要对一边横行和竖行的数字之和，作为校验。

词法是词的编码规则，而语法是语言的编码解码规则。词法的编码是有限的，而语法是可以无限扩展的。所以每个时代都有不符合原语法的“病句”而这些语句后来往往流传下去成为新的语法。

<br/>
### 第2章 自然语言处理（从规则到统计）
最初专家们（包括香农）认为要进行语言翻译，就要先要实现计算机类似人脑的智能，就如一个能对两种语言翻译的人一定是对两种语言都能理解的人。

后来人们对这种靠直觉判断的方法论称为“飞鸟派”，意思是看到鸟扇翅膀就认为要想飞就得让人扇翅膀，而实际上莱特兄弟靠的是空气动力学而不是仿生学。

同样的，要实现机器翻译，靠得并非是计算机模拟人工智能，靠得是数学，更准确地说是靠统计。

计算机语言很容易被计算机理解，所以人们就想自然语言文法也能用相同的方法分析，而实际上计算机语言是上下文无关的。

图灵奖得主高德纳提出用计算复杂度来衡量算法的耗时。对于上下文无关的文法，复杂度大概是语句长度的二次方，而上下文有关的自然语言的文法复杂度是语句长度的六次方。即使是现在要翻译一句较长的自然语言要计算

<br/>
### 第3章 统计语言模型
基本的统计模型就是选取概率最高的句子，一个句子的概率就是每个词在前一个词之后的条件概率之积。

在实际操作时，会用一些简化模型，效果也是不错的。

如果语料库足够大，翻译准确率就会很高。

<br/>
### 第4章 谈谈分词
如“北京大学”，是应该分成一个、两个还是四个词就是分词问题。较为好的分词方式是按照统计概率和条件概率取得的结果。

<br/>
### 第5章 隐含马尔可夫模型
计算一系列词出现的概率时原本是一系列条件概率模型，但是这种模型过于复杂，隐含马尔可夫模型是把概率简化为每一个词出现的概率只与前面一个词有关，这样便于计算。

围绕隐含马尔可夫模型有三个基本问题：
* 1.给定一个模型，如何计算某个特定输出序列的概率。
* 2.给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列。
* 3.给定足够量的观测数据，如何估计隐含马尔可夫模型的参数。

第一个问题对应的算法是Forward-Backward算法。

第二个问题可以用维特比算法解决。

第三个问题可以利用鲍姆-韦尔奇算法进行模型训练。

<br/>
### 第6章 信息的度量和作用
* 1 信息熵 entropy

一段信息的不确定性可以用信息熵描述，用来度量信息，熵越大信息量越小。

* 2 信息的作用

信息的作用就是通过增加冗余来降低信息熵。词之间通过条件概率降低信息熵。

* 3 互信息 mutual information

互信息就是两个信息之间的概率相关度，在0～1之间，0代表不相关，1代表完全相关。

有足够的语料库，计算概率和相关信息条件概率都是很容易的事。

* 4 相对熵

<br/>

