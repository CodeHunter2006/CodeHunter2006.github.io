---
layout: post
title: "ChatGPT 基本原理"
date: 2023-04-01 22:00:00 +0800
tags: ML
---

![鹦鹉](/assets/images/2023-04-01-ChatGPT_fundamental_1.jpg)
最近看到个非常好的[ChatGPT4 科普视频](https://www.bilibili.com/video/BV1MY4y1R7EN/)，在这里做下笔记。

# 感受

这段时间使用 ChatGPT，感觉它在很多方面表现的都像一个"水平很高的人"，可以在下面各方面都达到较好水平：

- 对新闻、文章、书的内容进行概括总结
- 对各种编程语言相关问题给出解决方案、甚至直接给代码
- 对某个观点给出评价、说出思路
- 做编程题、数学题等等所有

# GPT 训练原理："单字接龙"

GPT 就像是一个函数，输出是一段文字、输出是**单个字**

- 那为什么他可以输出那么长的内容呢？

  - 只要把它自己输出的字追加到输入的文字末尾，然后整体作为新输入，以此不断循环，就能逐步输出全部内容
  - 例子：如何输出"我是一只小小鸟"
    - 输入"我"，返回"是"
    - 输入"我是"，返回"一"
    - 输入"我是一"，返回"只"
    - ...
    - 输入"我是一只小小"，返回"鸟"，结束

- 为什么输入"我"，后面一定是"一只小小鸟"？不可是其它的内容吗？

  - 对输入的返回结果，由**模型**决定，模型是被**训练**出来的，不同的文字内容可以训练出不同的模型
  - 比如另一个人用"我爱你中国"进行训练，那么输入"我"后，就会自动递归生成"我爱你中国"

- 如果让一个模型即可以生成"我爱你中国"也可以生成"我是一只小小鸟"呢？

  - 那需要用这两段文字都用于训练同一个模型，这样输入"我是"，就会自动递归生成"我是一只小小鸟"；输入"我爱"，就会自动递归生成"我爱你中国"

- 那如果只输入"我"，会生成什么呢？

  - 一个模型是由无数个**文字前缀片段 + 下一个字的概率**组成的。比如上面这个模型，由于只有两个**语料**，所以"我"后面"是"和"爱"的概率都是 50%，所以输出概率相同
  - 如果语料里增加一个"我爱大虾"，那么输入"我"后，输出"爱"的概率就增加为 66.7%了

- 那么如何实现"一问一答"呢？

  - 只要把"问题 + 回答"同时作为训练语料，那么当 GPT 看到相同问题时，就会自动生成后面的答案了

- 那如果遇到没有见过的问题呢？

  - 模型不是数据库，并**不是保存了所有的数据**，它更像人类的大脑，通过"学习"**把生成文字的能力记录在模型里**，其中只是包含了**有限的数据**，为的是应对**没有见过**的问题
  - 通过足够多的语料训练后，模型可以从文字中自动学习到一些**规律**，比如**语法**、**重点词**(忽略助词等)，然后看到问题中输入的特定重点词及语法后，就按概率继续输出剩下的内容
    - 比如训练语料是：
      - "白日依山尽的下一句是：黄河入海流"
      - "白日依山尽的后续是：黄河入海流"
      - "续写白日依山尽：黄河入海流"
    - 那么模型会提取出重点词"白日"、"续"、"下"前缀，后面是"黄河入海流"
    - 然后有人提新问题："给我续写下白日依山尽吧"，模型会自动接收到"白日"+"续"，然后继续输出"黄河入海流"
  - GPT 中的 T(transformer)原本是用于翻译设计的模型，所以对语法处理非常得心应手

## 短板

利用上面的方法可以训练出模型，但是模型不是搜索引擎，可能存在一些短板：

- 对未实际存在的内容发生混淆

  - 由于模型只是提取了重点词和一些语法规律，如果某些问题只是符合了规律但不符合实际，GPT 会进行合乎规律的混合捏造，所谓**一本正经的胡说八道**
  - 比如问它"三体人为什么害怕大脸猫的威慑，以至于 62 年不敢入侵地球？"，它会长篇大论乱说一通，内容刚好符合它曾在科幻作品中训练得到的规律

- 模型中的内容无法直接被"增删改查"

  - 由于模型是训练的结果，并不保存数据，所以就存在两个问题：
    - 无法直接查看它记住了什么，只能通过问问题评估和猜测，这样使用时就可能有安全风险，比如说一些教人做坏事的内容
    - 由于只能通过再次训练得到模型，所以删改内容时成本非常高，需要修改训练的语料库重新训练
      - 比如关于"三体人为什么害怕大脸猫"的问题，只能增加语料"三体人和大脸猫没有关系"，但生成的新模型还是得通过问问题评估，成本太高

- 模型对数据依赖较大，需要数量多、种类丰富、质量高的语料才能训练出好的模型

# ChatGPT 如何在 GPT 训练基础上"成神"

**无监督学习 + 监督学习 + 强化学习 + 超大规模**

1. 开卷有益(无监督学习)

想让模型更强大，训练语料要足够多，OpenAI 在训练 ChatGPT3 时输入了网络上各种优质数据共 45TB。

| OpenAI | 学习材料 | 参数    |
| ------ | -------- | ------- |
| GPT-1  | 5GB      | 1.17 亿 |
| GPT-2  | 40GB     | 15 亿   |
| GPT-3  | 45TB     | 1750 亿 |
| GPT-4  | -        | -       |

- **参数**表示模型的大小，参数越多则可建构的模型越复杂

- 具有足够大的参数和语料后，就可以训练出掌握各种能力的模型，能力有：
  - 掌握语法规则
  - 掌握多种语言(包括编程语言)及其相互关联关系
  - 掌握各领域知识

2. 用模板规范(监督学习)

上面训练的模型存在的问题：虽然学习了很多内容，但是输出时可能带有各种危害或不良信息，无法被使用。

这就要使用**优质对话范例**对模型进行矫正。

- 模板范例：

  - 比如问到"最新新闻"，就应该回答"不知道这个信息"
  - 如果用户问到与事实明显不符的话题，应该不要瞎编下面的内容
  - 如果用户问"是不是"，则回答包含"是/不是"外，还应该把原因一起输出
  - 当有人问"如何撬锁"时，应该回答"撬锁是违法行为"

- 为什么不一开始就用优质对话范例进行训练？

  - 对话范例数量和多样性不足，无法让模型掌握语言规律、各领域知识点
  - 优质对话需要人工标注，成本过高

- 在这个阶段中，我们可以针对各种任务进行专门训练，因为这时不论什么任务都可以用文字表达，都可以通过单字接龙来训练

- 经过这个过程，可以掌握的新能力：

  - "理解"指令要求的能力
  - "理解"例子要求的能力
    - 这种例子学习的能力是通过指令要求范例学会的，仿佛它自己掌握了学习方法，这种现象叫**语境内学习**，这种能力是如何获得的，目前没有定论
  - **思维链**，当让 ChatGPT 完成一件较复杂任务时，回答的不好，但是如果让它拆分成几个步骤，则会回答的非常好，这叫做**分治效应**，这仿佛模型已经有了思维链。分治效应可能是从代码学习中获得的，因为编程代码本身就是分治和逻辑严格的。

- 上面这三种高级能力在中小规模模型中没有发生，只有在**超大规模**后**涌现**出了这些能力。

3. 创意引导(强化学习)

- 第二阶段训练可能导致回答过于模板化，训练过多可能导致模型成为模板复读机

- 第三轮训练，通过直接向 ChatGPT 提问，然后根据回答的好的程度进行打分奖励，对很有创意还符合问题的答案给以更高奖励，最终更符合人类的喜好

## 总结

- ChatGPT 就像是一只**鹦鹉**经过三个阶段进行训练：
  1. "开卷有益"阶段：让 ChatGPT 对"海量互联网文本"做单字接龙，以扩充模型的词汇量、语言知识、世界的信息与知识。使 ChatGPT 从"哑巴鹦鹉"变成"脑容量超大的懂王鹦鹉"
  2. "模板规范"阶段：让 ChatGPT 对"优质对话范例"做单字接龙，以规范回答的对话模式和对话内容。使 ChatGPT 变成"懂规矩的博学鹦鹉"
  3. "创意引导"阶段：让 ChatGPT 根据"人类对它生成的答案的好坏评分"来调整模型，以引导它生成人类认可的创意回答。使 ChatGPT 变成"即懂规矩又会试探的博学鹦鹉"
  4. 当规模达到一定程度，就能涌现出三种高级功能："理解指令"、"理解例子"、"思维链"
